---
layout:     post   				    # 使用的布局（不需要改）
title:      My First Post 				# 标题 
subtitle:   Hello World, Hello Blog #副标题
date:       2024-09-13 				# 时间
author:     马子程					# 作者
header-img: img/test_wukong.png 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 生活
usemath: latex
---

## 前言
>这是我的第一篇博客。

博客需要遵循特定的格式，比如YYYY-MM-DD-title.md，才会在网站上出现

同步远程修改
```bash
git pull origin master 
```

vscode提交到github出现圆圈卡顿的解决方案：参考

链接语法：
这是一个链接 [vscode提交卡顿](https://blog.csdn.net/lsfhack/article/details/131113277)。


gittalk评论配置教程：主要是修改_config.yml中的内容

[gitalk评论配置](https://yuanlichenai.cn/2020/01/16/Gitalk/#:~:text=Gitalk%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B%201%20%E4%BD%BF%E7%94%A8%20GitHub%20%E7%99%BB%E5%BD%95%202%20%E6%94%AF%E6%8C%81%E5%A4%9A%E8%AF%AD%E8%A8%80%20%5Ben%2C,%E4%B8%BA%20true%20%E5%BC%80%E5%90%AF%EF%BC%89%205%20%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%8F%90%E4%BA%A4%E8%AF%84%E8%AE%BA%20%EF%BC%88cmd%7Cctrl%20%2B%20enter%EF%BC%89)。


## 公式测试
罗马字母，公式书写方法与latex基本相同,需要在_config.yml中添加：
kramdown:
  math_engine: MathJax

然后在_includes/head.html中添加：
```
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
        }
        });
</script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>

```

还需要在每篇文章中的front matter中添加：
usemath: latex

模型的参数为$\alpha=1$

参考的公式为<br />

$$
\frac{a}{b} = \int_{a}^{b}f(x)dx 
$$
<br />
$$
\beta = 1,\alpha=2
$$

## 上传图片测试

随便上传一张头像,注意路径要用全局路径/img/acatar_m.jpg
![这是图片](/img/avatar_m.jpg "Magic Gardens")

## python代码测试

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig

tokenizer = AutoTokenizer.from_pretrained('/lustre/grp/gyqlab/mazc/data/LLaMA-Factory/Meta-Llama-3-8B-Instruct', use_fast=False, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained('/lustre/grp/gyqlab/mazc/data/LLaMA-Factory/Meta-Llama-3-8B-Instruct', device_map="auto",torch_dtype=torch.float16)

print("Hello, World!")
```